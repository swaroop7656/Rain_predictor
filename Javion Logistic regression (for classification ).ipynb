{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Logistic  Regression (classification)\n",
    "\n",
    "**unlike linear, logistic regression is predominently used to in classification problems. where as linear is used for the continuous data\n",
    "For instace....\n",
    "1. to predict whether tommorow will rain or not\n",
    "2. Based on prior transaction, shall a bank lend the loan to the perticular customer or not \n",
    "3. spam prediction is also a classification problem. that can be solved by logistic reg\n",
    "4. More than two option to make descision, such as classification of numbers \n",
    "\n",
    "**Majorly in case of linear regression we take all faetures and apply weights to them (weighted sum ) along with the biase finally get a result. But in case of of logistic regression we go further and put the weighted sum result to activation function called ***\"sigmoid\"*** **which pushes whole and gives result in between 0 and 1.\n",
    "\n",
    "$$ \\sigma(z) = \\Bigg( \\frac {1}{1+e^{-z}} \\bigg)$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [],
   "source": [
    "url='https://www.kaggle.com/jsphyg/weather-dataset-rattle-package'\n",
    "\n",
    "from urllib.request import urlopen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'path of json file'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-148-3bb489f2a285>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mjson\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[1;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'path of json file'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m'r'\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m     \u001b[0mdata\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mjson\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'username'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'path of json file'"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "with open('path of json file','r') as f:\n",
    "    data=json.load(f)\n",
    "    print(data['username'])\n",
    "\n",
    "\n",
    "key='**********************'\n",
    "username='xxxxxxxx'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "### library to download datasets from kaggle (easy approach)\n",
    "#!pip install opendatasets --upgrade --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import opendatasets as od\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#od.download(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "od.download(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path='.\\weather-dataset-rattle-package'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.listdir(data_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path=data_path+'/weatherAUS.csv'\n",
    "path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "data=pd.read_csv('.//weather-dataset-rattle-package/weatherAUS.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_row', 50)\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.Cloud9am.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.dropna(subset=['RainToday','RainTomorrow'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exploratory Data Analysis\n",
    "**it is neccessary to perform EDA before fitting data into model for training**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import plotly.express as px\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set_style('darkgrid')\n",
    "matplotlib.rcParams['font.size']=16\n",
    "matplotlib.rcParams['figure.figsize']=(14,6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "fig=px.histogram(data,x='Location', color='RainToday', marginal='box', title='rainyday based on location')\n",
    "\n",
    "fig.update_layout(bargap=0.2)\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig=px.histogram(data, x='Temp3pm', color='RainTomorrow', title='temperature at 3pm vs Rain tomorrow')\n",
    "fig.update_layout(bargap=0.1)\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.histplot(data=data, x='Temp3pm', hue='RainTomorrow',kde=True);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "px.histogram(data, x='RainTomorrow', color='RainToday', title='Rain today vs Rain Tomorrow')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "px.scatter(data.sample(2000), x='MinTemp',y='MaxTemp',title='max and mini temperature comparison',color='RainToday')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## observtion \n",
    "**form plotting max and mini temperature with respect to RainToday, we can infer that, there is not much difference in the max and min temperaure of that day when it was rained.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\"\"\n",
    "we specified 20% of data form whole as test , but we did not specified which part of 20% hence the \"random_state\" has been\n",
    "set as argument which normally provoke random number generator to pick the data from data set\n",
    "\"\"\"\"\"\n",
    "train_val, test=train_test_split(data, test_size=0.20, random_state=42)\n",
    "train, validate=train_test_split(train_val, test_size=0.25, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\033[1m training data :\", train.shape)\n",
    "print(\"validation data :\", validate.shape)\n",
    "print(\"test data :\",test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.countplot(x=pd.to_datetime(data.Date).dt.year)\n",
    "plt.title('No of rows per year')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### observation \n",
    "\n",
    "**However as we split the whole dataset into train, validate and test datasets, where in, each have mixed records with respect to Date** <br>\n",
    "**At the end, we train model with train dataset which will be having records collected from every year(because data is not filtered with time) and same goes with validation and test data. But here our main objective of building ML model is to predict feature events occurance based on past events data.Hence the train data is filtered with records merely collected before the data records present in test dataset.......**\n",
    "\n",
    " \n",
    "train->contain data records below 2015<br> \n",
    "validation->data collected at 2015<br> \n",
    "test->data collected after 2015\n",
    " \n",
    "***NOTE: \"Below filterring of data will give more understandability\"***\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### ignoring data split done prior\n",
    "year=pd.to_datetime(data.Date).dt.year\n",
    "\n",
    "train=data[year<2015]\n",
    "validate=data[year==2015]\n",
    "test=data[year>2015]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### we choosing columns from datasets, where redundant  columns were ignored\n",
    "\n",
    "input_cols=list(data.columns[1:-1])\n",
    "## target column selection\n",
    "target_col='RainTomorrow'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_input=train[input_cols].copy()\n",
    "train_target=train[target_col].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_input=test[input_cols].copy()\n",
    "test_target=test[target_col].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_input=validate[input_cols].copy()\n",
    "val_target=validate[target_col].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 'RainTomorrow'column is ignored along with the redundant\n",
    "val_input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##  'RainTomorrow' column records in pandas series (type)\n",
    "val_target"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### points to remember\n",
    "\n",
    "1. Ignored Date column because it does not contribute to the final result\n",
    "2. For instance if we have rainfall_Tomorrow column in our datasets, it should be ignored too. Since we are predicting whether it will rain or not\n",
    "3. We forging our model in accordance of location in the dataset (i,e in 49 locations data has been recorded), hence location place important role as each location have thier own unique climate attribute.<br>\n",
    "***MOST IMPORTANTLY, THIS MODEL WILL WORK WELL ONLY WITH THE INPUTS MERELY HAVING THESE LOCATIONS. HOWEVER TO GENERALIZE (works irrespective of location) THE MODEL WE NEED TO IGNORE LOCATION COLUMN.(moreover these data is inadequate to generalize the model)*** "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####   saparating  numerical data and categorical data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "numeric_cols= train_input.select_dtypes(include=np.number).columns.tolist()\n",
    "categorical=train_input.select_dtypes('object').columns.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_input[numeric_cols].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "train_input[categorical].nunique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Dealing with missing values (null/nNaN)\n",
    "**The process of filling missing values with valid values is called** ***IMPUTING*** <br>\n",
    "1. there are many different approches of imputing one such is filling with **mean value** of that column\n",
    "2. using ***median*** is also a best approach because sometimes there will be outliers which may affect the average as whole."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Sklearn imputer class is called for imputing\n",
    "\n",
    "from sklearn.impute import SimpleImputer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imputer=SimpleImputer(strategy='mean')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data[numeric_cols].isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_input[numeric_cols].isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###imputer will return specified statistic (in this case mean) value\n",
    "### remember it will not replace the missing value with stats value\n",
    "imputer.fit(data[numeric_cols])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "imputer.statistics_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imputer.transform(train_input[numeric_cols])\n",
    "## above imputer just created numpy array but we need it in our dataframe hence we need to overwrite with these values of array "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### overwriting \n",
    "\n",
    "train_input[numeric_cols]=imputer.transform(train_input[numeric_cols])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_input[numeric_cols].head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_input[numeric_cols]=imputer.transform(val_input[numeric_cols])\n",
    "val_input[numeric_cols].head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_input[numeric_cols]=imputer.transform(test_input[numeric_cols])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### scaling numeric columns\n",
    "**Numeric columns are scaled to small range values. where it is neccessary to scale the data in order to avoid the disproportionate effect of perticular feature on model's loss, and to avoid the adverse effect of optimizer "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "?MinMaxScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler=MinMaxScaler()\n",
    "\n",
    "scaler.fit(data[numeric_cols])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "scaler.data_min_ , scaler.data_max_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_input[numeric_cols]=scaler.transform(train_input[numeric_cols])\n",
    "val_input[numeric_cols]=scaler.transform(val_input[numeric_cols])\n",
    "test_input[numeric_cols]=scaler.transform(test_input[numeric_cols])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_input[numeric_cols]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_input[numeric_cols].describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lets deal with the categorical columns \n",
    "**Our obvious approach.... either  one hot encoding or encoding** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### having comprehensive view on categorical\n",
    "\n",
    "data[categorical].nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.Location.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encode=OneHotEncoder(sparse=False, handle_unknown='ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### dealing with nan values\n",
    "data_2=data[categorical].fillna('unkown')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encode.fit(data_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encode.categories_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "##generating column names for each columns by using get_feature_names method of sklearn\n",
    "\n",
    "encode_cols=list(encode.get_feature_names(categorical))\n",
    "\n",
    "for i in encode_cols:\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_input_2=train_input[categorical].fillna('unknown')\n",
    "train_input[encode_cols]=encode.transform(train_input_2)\n",
    "\n",
    "val_input_2=val_input[categorical].fillna('unknown')\n",
    "val_input[encode_cols]=encode.transform(val_input_2)\n",
    "\n",
    "\n",
    "test_input[encode_cols]=encode.transform(test_input[categorical].fillna('unknown'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "## to see all columns\n",
    "pd.set_option('max_columns',None)\n",
    "train_input.head(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##lets look into the shape of each data chunk\n",
    "\n",
    "print('\\033[1m Train input :', train_input.shape)\n",
    "print('\\033[1m Train target :', train_target.shape)\n",
    "print('\\033[1m validation input :', val_input.shape)\n",
    "print('\\033[1m validation traget :', val_target.shape)\n",
    "print('\\033[1m test input :', test_input.shape)\n",
    "print('\\033[1m Test target :', test_target.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\"\n",
    "it is optional!!! \n",
    "after all data preprocessing, if we need clean data to be used for another project or to train another model, we can store \n",
    "it as csv or another efficient format is parquet, for that we need to install pyarrow\n",
    "\"\"\"\"\"\n",
    "#!pip install pyarrow --upgrade --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_input.to_parquet('train_input.parquet')\n",
    "val_input.to_parquet('val_input.parquet')\n",
    "test_input.to_parquet('test_input.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "train_in=pd.read_parquet('train_input.parquet')\n",
    "train_in.tail(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Finally our datasets are ready to be used to train our reggression model\n",
    "$$ \\logistic \\space \\regression $$\n",
    "1. first we take weighted sum of each input features, as we do in liner regression\n",
    "2. Result set from the above set is fed into the the sigmoid function, which gives the result either 0 or 1\n",
    "3. and at lost, to reduce the cost function we use cross entropy loss function instead of RMSE\n",
    "**Sigmoid**\n",
    "$$\\sigma(x)= \\frac {1} {1+e^{-x}}$$\n",
    "\n",
    "**cross entorpy loss function**\n",
    "$$L(y^{*},y) = -y log y^{*} + (1-y)log(1-y^{*})$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 'liblinear is the optimization '\n",
    "model=LogisticRegression(solver='liblinear')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "### only nummeric columns need to be fed into the model in case of inputs \n",
    "## but in case of targets sklearn will covert the categories into numeric accordingly \n",
    "model.fit(train_input[numeric_cols + encode_cols], train_target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(list(numeric_cols+encode_cols))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##weights of each \n",
    "print(list(model.coef_)[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print( model.intercept_.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('max_rows',None)\n",
    "weights=pd.DataFrame({\n",
    "    'features': numeric_cols + encode_cols [:len(numeric_cols + encode_cols)],\n",
    "    'weights' : model.coef_[0]\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(5,5))\n",
    "sns.barplot(data=weights.sort_values('weights',ascending=False).head(10), x='weights', y='features')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.barplot(data=weights.sort_values('weights',ascending=True).head(10), x='weights', y='features')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_input.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train=train_input[numeric_cols+encode_cols]\n",
    "\n",
    "x_val=val_input[numeric_cols+encode_cols]\n",
    "\n",
    "x_test=test_input[numeric_cols+encode_cols]\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### model prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction=model.predict(x_train)\n",
    "train_target.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "comp_df=pd.DataFrame({\n",
    "    'Actual' : train_target,\n",
    "    'prediction': prediction\n",
    "})\n",
    "px.histogram(comp_df,x='Actual', color='prediction')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy_score(train_target,prediction)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## using a probability for prediction is a good meaure of confidence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###probabilistic prediction,and this is only for logistic regressiion\n",
    "\n",
    "train_prob=model.predict_proba(x_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_prob,model.classes_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## confusion Matrix\n",
    "\n",
    "***True postive-->> prediction is true and actual is also true*** <br>\n",
    "***True negative-->> prediction is false and the actual is also false*** <br>\n",
    "**False positive--->> prediction is true but actual is false (Type1 error) ** <br>\n",
    "**False negative --->> prediction is false but actual is true Type2 error)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### confusion matrix\n",
    "\n",
    "from sklearn.metrics import confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "confusion_matrix(train_target,prediction, normalize='true')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy_cofmatrix(data_input,data_target, name=''):\n",
    "    prediction=model.predict(data_input)\n",
    "    accuracy=accuracy_score(data_target, prediction)\n",
    "    print('\\033[1m accuracy:',accuracy*100,'%')\n",
    "    cf=confusion_matrix(data_target,prediction, normalize='true')\n",
    "    sns.heatmap(cf,annot=True)\n",
    "    plt.xlabel('prediction')\n",
    "    plt.ylabel('target')\n",
    "    plt.title('confusion matrix of {}'.format(name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy_cofmatrix(test_input[numeric_cols+encode_cols],test_target,'validation')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### lets use some base line models to compare with our model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_predict(inputs):\n",
    "    rand_pred=np.random.choice(['no','yes'],len(inputs))\n",
    "    return rand_pred\n",
    "    \n",
    "def all_yes(inputs):\n",
    "    return np.full(len(inputs),'yes')\n",
    "\n",
    "### given both our targets column with any of these functions ouput, find accuracy and compare with the model accuracy "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## lets give random input to our model (single input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_data={'Date':'2021-10-20',\n",
    "          'Location':'SydneyAirport',\n",
    "          'MinTemp':23.2,\n",
    "          'MaxTemp':33.4,\n",
    "          'Rainfall':10.2,\n",
    "          'Evaporation':4.8,\n",
    "          'Sunshine':np.nan,\n",
    "          'WindGustDir':'NNW',\n",
    "          'WindGustSpeed':10.0,\n",
    "          'WindDir9am':'NW',\n",
    "          'WindDir3pm':'NNE',\n",
    "          'WindSpeed9am':13.0, \n",
    "          'WindSpeed3pm':20.4, \n",
    "          'Humidity9am':89.2, \n",
    "          'Humidity3pm':58.0,\n",
    "          'Pressure9am':1000, \n",
    "          'Pressure3pm':1001.5,\n",
    "          'Cloud9am':8.0,  \n",
    "          'Cloud3pm':5.0, \n",
    "          'Temp9am':25.7,\n",
    "          'Temp3pm':33.0, \n",
    "          'RainToday':'Yes', \n",
    "          \n",
    "    \n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_df=pd.DataFrame([new_data])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Date</th>\n",
       "      <th>Location</th>\n",
       "      <th>MinTemp</th>\n",
       "      <th>MaxTemp</th>\n",
       "      <th>Rainfall</th>\n",
       "      <th>Evaporation</th>\n",
       "      <th>Sunshine</th>\n",
       "      <th>WindGustDir</th>\n",
       "      <th>WindGustSpeed</th>\n",
       "      <th>WindDir9am</th>\n",
       "      <th>WindDir3pm</th>\n",
       "      <th>WindSpeed9am</th>\n",
       "      <th>WindSpeed3pm</th>\n",
       "      <th>Humidity9am</th>\n",
       "      <th>Humidity3pm</th>\n",
       "      <th>Pressure9am</th>\n",
       "      <th>Pressure3pm</th>\n",
       "      <th>Cloud9am</th>\n",
       "      <th>Cloud3pm</th>\n",
       "      <th>Temp9am</th>\n",
       "      <th>Temp3pm</th>\n",
       "      <th>RainToday</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2021-10-20</td>\n",
       "      <td>SydneyAirport</td>\n",
       "      <td>23.2</td>\n",
       "      <td>33.4</td>\n",
       "      <td>10.2</td>\n",
       "      <td>4.8</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NNW</td>\n",
       "      <td>10.0</td>\n",
       "      <td>NW</td>\n",
       "      <td>NNE</td>\n",
       "      <td>13.0</td>\n",
       "      <td>20.4</td>\n",
       "      <td>89.2</td>\n",
       "      <td>58.0</td>\n",
       "      <td>1000</td>\n",
       "      <td>1001.5</td>\n",
       "      <td>8.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>25.7</td>\n",
       "      <td>33.0</td>\n",
       "      <td>Yes</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         Date       Location  MinTemp  MaxTemp  Rainfall  Evaporation  \\\n",
       "0  2021-10-20  SydneyAirport     23.2     33.4      10.2          4.8   \n",
       "\n",
       "   Sunshine WindGustDir  WindGustSpeed WindDir9am WindDir3pm  WindSpeed9am  \\\n",
       "0       NaN         NNW           10.0         NW        NNE          13.0   \n",
       "\n",
       "   WindSpeed3pm  Humidity9am  Humidity3pm  Pressure9am  Pressure3pm  Cloud9am  \\\n",
       "0          20.4         89.2         58.0         1000       1001.5       8.0   \n",
       "\n",
       "   Cloud3pm  Temp9am  Temp3pm RainToday  \n",
       "0       5.0     25.7     33.0       Yes  "
      ]
     },
     "execution_count": 206,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [],
   "source": [
    "#imputing to remove Nan values\n",
    "new_df[numeric_cols]=imputer.transform(new_df[numeric_cols])\n",
    "\n",
    "#scaling/standardizing column values to make it unifrom and to go easy while optimizing and while finding cost function\n",
    "new_df[numeric_cols]=scaler.transform(new_df[numeric_cols])\n",
    "\n",
    "#one hot encoding categorical columns\n",
    "new_df[encode_cols]=encode.transform(new_df[categorical])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['No'], dtype=object)"
      ]
     },
     "execution_count": 208,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prediction=model.predict(new_df[numeric_cols+encode_cols])\n",
    "\n",
    "prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[0.86995778, 0.13004222]]), array(['No', 'Yes'], dtype=object))"
      ]
     },
     "execution_count": 209,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prob=model.predict_proba(new_df[numeric_cols+encode_cols])\n",
    "\n",
    "prob, model.classes_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['Albury', 'BadgerysCreek', 'Cobar', 'CoffsHarbour', 'Moree',\n",
       "       'Newcastle', 'NorahHead', 'NorfolkIsland', 'Penrith', 'Richmond',\n",
       "       'Sydney', 'SydneyAirport', 'WaggaWagga', 'Williamtown',\n",
       "       'Wollongong', 'Canberra', 'Tuggeranong', 'MountGinini', 'Ballarat',\n",
       "       'Bendigo', 'Sale', 'MelbourneAirport', 'Melbourne', 'Mildura',\n",
       "       'Nhil', 'Portland', 'Watsonia', 'Dartmoor', 'Brisbane', 'Cairns',\n",
       "       'GoldCoast', 'Townsville', 'Adelaide', 'MountGambier', 'Nuriootpa',\n",
       "       'Woomera', 'Albany', 'Witchcliffe', 'PearceRAAF', 'PerthAirport',\n",
       "       'Perth', 'SalmonGums', 'Walpole', 'Hobart', 'Launceston',\n",
       "       'AliceSprings', 'Darwin', 'Katherine', 'Uluru'], dtype=object)"
      ]
     },
     "execution_count": 210,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## to fetch other locations name\n",
    "data.Location.unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## like this we can use random inputs(as we done above) to check how good the model is."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## lets save trained model and its parameters, so that we can use that later"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {},
   "outputs": [],
   "source": [
    "import joblib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {},
   "outputs": [],
   "source": [
    "Aust_predictor={\n",
    "    'model':model,\n",
    "    'imputer':imputer,\n",
    "    'scaler':scaler,\n",
    "    'encoder':encode,\n",
    "    'input_cols':input_cols,\n",
    "    'target_cols':target_col,\n",
    "    'numeric_cols':numeric_cols,\n",
    "    'categorical_cols':categorical,\n",
    "    'encode_cols':encode_cols\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['aust_prediction.joblib']"
      ]
     },
     "execution_count": 215,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "joblib.dump(Aust_predictor,'aust_prediction.joblib')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_1=joblib.load('aust_prediction.joblib')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression(solver='liblinear')"
      ]
     },
     "execution_count": 219,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_1['model']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### lets make some prediction using the model stored in the joblin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.842045896538312"
      ]
     },
     "execution_count": 221,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_predict=model_1['model'].predict(x_test)\n",
    "accuracy_score(test_target,test_predict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "usage: git [--version] [--help] [-C <path>] [-c <name>=<value>]\n",
      "           [--exec-path[=<path>]] [--html-path] [--man-path] [--info-path]\n",
      "           [-p | --paginate | -P | --no-pager] [--no-replace-objects] [--bare]\n",
      "           [--git-dir=<path>] [--work-tree=<path>] [--namespace=<name>]\n",
      "           <command> [<args>]\n",
      "\n",
      "These are common Git commands used in various situations:\n",
      "\n",
      "start a working area (see also: git help tutorial)\n",
      "   clone             Clone a repository into a new directory\n",
      "   init              Create an empty Git repository or reinitialize an existing one\n",
      "\n",
      "work on the current change (see also: git help everyday)\n",
      "   add               Add file contents to the index\n",
      "   mv                Move or rename a file, a directory, or a symlink\n",
      "   restore           Restore working tree files\n",
      "   rm                Remove files from the working tree and from the index\n",
      "   sparse-checkout   Initialize and modify the sparse-checkout\n",
      "\n",
      "examine the history and state (see also: git help revisions)\n",
      "   bisect            Use binary search to find the commit that introduced a bug\n",
      "   diff              Show changes between commits, commit and working tree, etc\n",
      "   grep              Print lines matching a pattern\n",
      "   log               Show commit logs\n",
      "   show              Show various types of objects\n",
      "   status            Show the working tree status\n",
      "\n",
      "grow, mark and tweak your common history\n",
      "   branch            List, create, or delete branches\n",
      "   commit            Record changes to the repository\n",
      "   merge             Join two or more development histories together\n",
      "   rebase            Reapply commits on top of another base tip\n",
      "   reset             Reset current HEAD to the specified state\n",
      "   switch            Switch branches\n",
      "   tag               Create, list, delete or verify a tag object signed with GPG\n",
      "\n",
      "collaborate (see also: git help workflows)\n",
      "   fetch             Download objects and refs from another repository\n",
      "   pull              Fetch from and integrate with another repository or a local branch\n",
      "   push              Update remote refs along with associated objects\n",
      "\n",
      "'git help -a' and 'git help -g' list available subcommands and some\n",
      "concept guides. See 'git help <command>' or 'git help <concept>'\n",
      "to read about a specific subcommand or concept.\n",
      "See 'git help git' for an overview of the system.\n"
     ]
    }
   ],
   "source": [
    "!git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
